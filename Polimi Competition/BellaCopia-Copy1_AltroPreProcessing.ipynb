{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe management\n",
    "import pandas as pd             \n",
    "\n",
    "# numerical computation\n",
    "import numpy as np\n",
    "\n",
    "# visualization library\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":'sans',\"font.size\":24,\"axes.titlesize\":24,\"axes.labelsize\":24})   \n",
    "\n",
    "\n",
    "# import matplotlib and allow it to plot inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#----------------------------------------------------------------------------------------------------\n",
    "import pyspark\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "from pyspark.sql.functions import udf,lit\n",
    "\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import mean\n",
    "\n",
    "sc = pyspark.SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "# seaborn can generate several warnings, we ignore them\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the notebook\n",
    "sales_string_date=pd.read_csv('train.csv')\n",
    "sales = sales_string_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation of Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_per_columns = sales.isnull().sum()\n",
    "unknown_per_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Max_Gust_SpeedKm_h has 409947 missing values. We decided not to impute it.\n",
    "Let's start with imputation of \"Events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_Events = sales['Events'].isnull()\n",
    "event_missing = sales[null_Events]\n",
    "event_missing.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Events'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this, we discover that when Event is null the weather is good, from the fact that Precipitationmm mean is almost 0.\n",
    "Furthermore, all the labels of Events are related to bad weather, that means that when no precipitation occurs the label is null.\n",
    "We will impute Event by replacing the missing values with \"Not Specified\" (later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we impute \"CloudCover\", making a distinction when it misses along with Events and when it misses on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_Events = sales['Events'].isnull()\n",
    "null_CloudCover = sales['CloudCover'].isnull()\n",
    "cloudcover_missing = sales[(null_Events)]\n",
    "null_Events = sales['Events'].isnull()\n",
    "event_missing = sales[null_Events]\n",
    "event_cc_missing = sales[null_CloudCover & null_Events]\n",
    "cloudcover_missing.shape, event_missing.shape, event_cc_missing.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 28k tuples where both \"Events\" and \"CloudCover\" are missing, that means that the weather should be good.\n",
    "for the remaining (41k-28k) (i.e. where \"Events\" is not null!) tuples we impute the CloudCoverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_notmissing_cc_missing = sales[null_CloudCover & ~null_Events]\n",
    "#computing mean where CloudCover is not null\n",
    "mean_CC = sales[\"CloudCover\"].mean()\n",
    "event_notmissing_cc_missing[\"CloudCover\"] = event_notmissing_cc_missing[\"CloudCover\"].fillna(mean_CC)\n",
    "sales = pd.concat([sales[~null_CloudCover | null_Events], event_notmissing_cc_missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we impute the remaining rows with CloudCover missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_Events = sales['Events'].isnull()\n",
    "CloudyButNotEvent = sales[null_Events]\n",
    "#there are some tuples with no Events but with the attribute CloudCover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cloudCover = sales[sales['CloudCover'].isnull()]\n",
    "mean_cloudCover = sales[\"CloudCover\"].mean()\n",
    "null_cloudCover[\"CloudCover\"] = null_cloudCover[\"CloudCover\"].fillna(mean_cloudCover)\n",
    "\n",
    "sales = pd.concat([sales[~sales['CloudCover'].isnull()], null_cloudCover])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we impute min,max,mean_VisibilityKm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if are all the same rows where visibility data are missing => yes, they are\n",
    "sales[[\"Max_VisibilityKm\", \"Mean_VisibilityKm\", \"Min_VisibilitykM\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_visibility = sales[sales['Max_VisibilityKm'].isnull()]\n",
    "mean_vis_max = sales[\"Max_VisibilityKm\"].mean()\n",
    "mean_vis_mean = sales[\"Mean_VisibilityKm\"].mean()\n",
    "mean_vis_min = sales[\"Min_VisibilitykM\"].mean()\n",
    "\n",
    "null_visibility[\"Max_VisibilityKm\"] = null_visibility[\"Max_VisibilityKm\"].fillna(mean_vis_max)\n",
    "null_visibility[\"Mean_VisibilityKm\"] = null_visibility[\"Mean_VisibilityKm\"].fillna(mean_vis_mean)\n",
    "null_visibility[\"Min_VisibilitykM\"] = null_visibility[\"Min_VisibilitykM\"].fillna(mean_vis_min)\n",
    "\n",
    "sales = pd.concat([sales[~sales['Max_VisibilityKm'].isnull()], null_visibility])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we impute \"Events\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales=sales.replace(np.nan,'NotSpecified', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask to normalize, avoiding categorical_varibles and target columns\n",
    "categorical_variables_mask = (sales.dtypes==object) | (sales.columns == \"Day\") | (sales.columns == \"Month\") | (sales.columns == \"Year\") | (sales.columns == \"StoreID\") | (sales.columns == \"IsOpen\") | (sales.columns == \"IsHoliday\") | (sales.columns == \"HasPromotions\") | (sales.columns == \"Region\"  )\n",
    "categorical_variables = categorical_variables_mask.tolist()\n",
    "numerical_variables = sales.columns[~categorical_variables_mask]\n",
    "normalization_mask = ~(categorical_variables | (sales.columns == \"IsOpen\") | (sales.columns == \"IsHoliday\") | (sales.columns == \"HasPromotions\") | (sales.columns == \"Region\"  ) | ( sales.columns == \"Max_Gust_SpeedKm_h\" ) | ( sales.columns == \"NumberOfSales\" ) | ( sales.columns == \"NumberOfCustomers\"))\n",
    "normalization_mask = normalization_mask.tolist()\n",
    "normalization_mask = sales.columns[normalization_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(sales[normalization_mask])\n",
    "df_normalized_values = scaler.transform(sales[normalization_mask])\n",
    "sales_normalized = pd.DataFrame(data = df_normalized_values, columns=normalization_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''cov=sales[numerical_variables].corr(method='pearson')\n",
    "cm = sns.clustermap(cov, annot=True, center=0, cmap=\"Blues\", figsize=(25, 25))\n",
    "cm.cax.set_visible(False)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the correlation, we see that the following variables do not add any additional information. \n",
    "Max_Dew_PointC, Min_Dew_PointC, Max_Sea_Level_PressurehPa, Mean_Sea_Level_PressurehPa, Max_Gust_SpeedKm_h\n",
    "Finally, we drop 'NumberOfCustomers' because is not present in the submission dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales=sales.drop(columns=['Max_Dew_PointC','Min_Dew_PointC','Max_Sea_Level_PressurehPa','Mean_Sea_Level_PressurehPa','Max_Gust_SpeedKm_h','NumberOfCustomers'])\n",
    "sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUI CI VA L'ANALISI STAGIONALE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flavio ci ha detto che non c'Ã¨ trend stagionale, quindi droppiamo le date e ci teniamo solo i giorni della settimana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import datetime as d\n",
    "def toDate(x):\n",
    "    parts = [int(el) for el in x.split(\"/\")]\n",
    "    return d.date(parts[2], parts[1], parts[0])\n",
    "sales[\"Date\"] = sales[\"Date\"].apply(toDate)\n",
    "sales['Date'] = sales['Date'].astype(\"datetime64\")\n",
    "sales['Day_Of_Week'] = sales['Date'].dt.weekday_name\n",
    "sales=sales.drop(columns=['Date'])'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sales = pd.get_dummies(sales, columns=['StoreType','Day_Of_Week','Events','AssortmentType',])\n",
    "sales.head(5)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try: MultipleLinear Regression for Region 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sales_train = sales.drop(['NumberOfSales'], axis=1)\n",
    "y = sales['NumberOfSales']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''salesRegion7_train = sales_train.loc[sales_train['Region'] == 7]\n",
    "salesRegion7_train.drop(columns='Region')\n",
    "salesRegion7_train.shape'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''toGetTarget = sales.loc[sales_train['Region'] == 7]\n",
    "y = toGetTarget['NumberOfSales']'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, ElasticNet, Lasso, LassoCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "def r2_cv(model, sales_train, y, random_state=12345678):\n",
    "    r2= cross_val_score(model, sales_train, y, scoring=\"r2\", cv =KFold(10, shuffle=True, random_state=random_state)) \n",
    "    return(r2)\n",
    "\n",
    "def rmse_cv(model, sales_train, y, random_state=12345678):\n",
    "    rmse= np.sqrt(-cross_val_score(model, sales_train, y, scoring=\"neg_mean_squared_error\", cv =KFold(10, shuffle=True, random_state=random_state)))\n",
    "    return(rmse)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''model_simple = LinearRegression()\n",
    "model_simple.fit(salesRegion7_train, y)\n",
    "yp = model_simple.predict(salesRegion7_train)\n",
    "\n",
    "# compute R2 for train and using crossvalidation\n",
    "r2_simple_train = r2_score(y,yp)\n",
    "r2_xval_simple =  r2_cv(model_simple, salesRegion7_train, y)\n",
    "\n",
    "# compute RMSE for train and using crossvalidation\n",
    "rmse_simple_train = mean_squared_error(y,yp,multioutput='raw_values')\n",
    "rmse_xval_simple =  rmse_cv(model_simple, salesRegion7_train, y)\n",
    "\n",
    "print(\"Linear Regression\")\n",
    "print(\"==================================================\")\n",
    "print(\"\\t                  Train R2=%.3f\"%(r2_simple_train))\n",
    "print(\"\\t10-fold Crossvalidation R2=%.3f\"%(r2_xval_simple.mean()))\n",
    "print(\"\\t                  Train RMSE=%.3f\"%(rmse_simple_train))\n",
    "print(\"\\t10-fold Crossvalidation RMSE=%.3f\"%(rmse_xval_simple.mean()))'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Winsorizing of  NearestCompetitor on 95th percentile\n",
    "#Before\n",
    "avgSalesForCompDist=sales\n",
    "avgSalesForCompDist.groupby('NearestCompetitor', as_index=False)['NumberOfSales'].mean()\n",
    "x = avgSalesForCompDist['NearestCompetitor']\n",
    "y = avgSalesForCompDist['NumberOfSales']\n",
    "font = {'family' : 'sans', 'size'   : 16}\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rc('font', **font)\n",
    "#plt.ylim((1.75,7.25))\n",
    "#plt.xlim((10,80))\n",
    "plt.xlabel(\"NearestCompetitor\")\n",
    "plt.ylabel(\"Average NumberOfSales\")\n",
    "plt.scatter(x, y, color='blue', marker='o', s=5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index1=np.where(sales['NearestCompetitor']>=NC95p)\n",
    "#sales['NearestCompetitor'][index1]\n",
    "\n",
    "NC95p=sales['NearestCompetitor'].quantile(.95)\n",
    "sales.loc[sales[\"NearestCompetitor\"]>NC95p,\"NearestCompetitor\"]=NC95p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Winsorizing of Precipitationmm on 95th percentile\n",
    "#Before\n",
    "avgSalesForCompDist=sales\n",
    "avgSalesForCompDist.groupby('Precipitationmm', as_index=False)['NumberOfSales'].mean()\n",
    "x = avgSalesForCompDist['Precipitationmm']\n",
    "y = avgSalesForCompDist['NumberOfSales']\n",
    "font = {'family' : 'sans', 'size'   : 16}\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rc('font', **font)\n",
    "#plt.ylim((1.75,7.25))\n",
    "#plt.xlim((10,80))\n",
    "plt.xlabel(\"Precipitationmm\")\n",
    "plt.ylabel(\"Average NumberOfSales\")\n",
    "plt.scatter(x, y, color='blue', marker='o', s=5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=sales['Precipitationmm'].quantile(.99)\n",
    "sales.loc[sales[\"Precipitationmm\"]>temp,\"Precipitationmm\"]=temp\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After\n",
    "avgSalesForCompDist=sales\n",
    "avgSalesForCompDist.groupby('Precipitationmm', as_index=False)['NumberOfSales'].mean()\n",
    "x = avgSalesForCompDist['Precipitationmm']\n",
    "y = avgSalesForCompDist['NumberOfSales']\n",
    "font = {'family' : 'sans', 'size'   : 16}\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.rc('font', **font)\n",
    "#plt.ylim((1.75,7.25))\n",
    "#plt.xlim((10,80))\n",
    "plt.xlabel(\"Precipitationmm\")\n",
    "plt.ylabel(\"Average NumberOfSales\")\n",
    "plt.scatter(x, y, color='blue', marker='o', s=5)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=sales['Max_Wind_SpeedKm_h'].quantile(.99)\n",
    "sales.loc[sales[\"Max_Wind_SpeedKm_h\"]>temp,\"Max_Wind_SpeedKm_h\"]=temp\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=sales['Max_Wind_SpeedKm_h'].quantile(.01)\n",
    "sales.loc[sales[\"Max_Wind_SpeedKm_h\"]<temp,\"Max_Wind_SpeedKm_h\"]=temp\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=sales['Max_TemperatureC'].quantile(.99)\n",
    "sales.loc[sales[\"Max_TemperatureC\"]>temp,\"Max_TemperatureC\"]=temp\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=sales['Max_TemperatureC'].quantile(.01)\n",
    "sales.loc[sales[\"Max_TemperatureC\"]<temp,\"Max_TemperatureC\"]=temp\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Min_TemperatureC'\n",
    "temp=sales[x].quantile(.01)\n",
    "sales.loc[sales[x]<temp,x]=temp\n",
    "temp=sales[x].quantile(.99)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Mean_Dew_PointC'\n",
    "temp=sales[x].quantile(.05)\n",
    "sales.loc[sales[x]<temp,x]=temp\n",
    "temp=sales[x].quantile(.98)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Mean_Humidity'\n",
    "temp=sales[x].quantile(.01)\n",
    "sales.loc[sales[x]<temp,x]=temp\n",
    "temp=sales[x].quantile(.95)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Min_VisibilitykM'\n",
    "temp=sales[x].quantile(.98)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Min_Humidity'\n",
    "temp=sales[x].quantile(.02)\n",
    "sales.loc[sales[x]<temp,x]=temp\n",
    "temp=sales[x].quantile(.98)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Mean_Wind_SpeedKm_h'\n",
    "temp=sales[x].quantile(.99)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Mean_TemperatureC'\n",
    "temp=sales[x].quantile(.01)\n",
    "sales.loc[sales[x]<temp,x]=temp\n",
    "temp=sales[x].quantile(.99)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='Mean_VisibilityKm'\n",
    "temp=sales[x].quantile(.01)\n",
    "sales.loc[sales[x]<temp,x]=temp\n",
    "temp=sales[x].quantile(.95)\n",
    "sales.loc[sales[x]>temp,x]=temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Min_Sea_Level_PressurehPa  ->dropparla\n",
    "#WindDirDegrees             -> inutile secondo me -> o la si categorizza in nord sud est ovest e la si fa con One Hot Encoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=\"\"\n",
    "#sales[x].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales[x].quantile(.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sales[x].quantile(.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
