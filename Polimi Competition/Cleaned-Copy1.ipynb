{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe management\n",
    "import pandas as pd             \n",
    "\n",
    "# numerical computation\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "# visualization library\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set_context(rc={\"font.family\":'sans',\"font.size\":24,\"axes.titlesize\":24,\"axes.labelsize\":24})   \n",
    "\n",
    "\n",
    "# import matplotlib and allow it to plot inline\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# seaborn can generate several warnings, we ignore them\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from bokeh.layouts import gridplot\n",
    "from bokeh.plotting import figure\n",
    "\n",
    "from bokeh.io import output_notebook, show\n",
    "from bokeh.models import ColumnDataSource\n",
    "output_notebook()\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "from scipy.stats import skew\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV, ElasticNet, Lasso, LassoCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toDate(x):\n",
    "    parts = [int(el) for el in x.split(\"/\")]\n",
    "    return date(parts[2], parts[1], parts[0])\n",
    "\n",
    "def r2_cv(model, sales_train, y, random_state=12345678):\n",
    "    r2= cross_val_score(model, sales_train, y, scoring=\"r2\", cv =KFold(10, shuffle=True, random_state=random_state)) \n",
    "    return(r2)\n",
    "\n",
    "def rmse_cv(model, sales_train, y, random_state=12345678):\n",
    "    rmse= np.sqrt(-cross_val_score(model, sales_train, y, scoring=\"neg_mean_squared_error\", cv =KFold(10, shuffle=True, random_state=random_state)))\n",
    "    return(rmse)\n",
    "\n",
    "def RegionError(region, data):\n",
    "    d = data[data[region] == 1][[\"StoreID\",\"Month\",\"NumberOfSales\",\"NumberOfPredictedSales\"]].groupby([\"StoreID\",\"Month\"]).agg(\"sum\")\n",
    "    res = abs(d[\"NumberOfSales\"]-d[\"NumberOfPredictedSales\"]).agg(\"sum\")\n",
    "    return res / d[\"NumberOfSales\"].agg(\"sum\")\n",
    "\n",
    "def q(col, quant, f):\n",
    "    t = sales[col].quantile(quant)\n",
    "    print(f'col {col} at {quant}-th quantile => {t}')\n",
    "    sales.loc[f(sales[col], t), col] = t\n",
    "    \n",
    "def getFilterRegion(cluster, data):\n",
    "    filterRegion = data[cluster[0]] == 1\n",
    "    for region in cluster[1:]:\n",
    "        filterRegion = ((filterRegion) | (data[region]==1))\n",
    "    return filterRegion\n",
    "\n",
    "def getColsMatching(data, oldCol):\n",
    "    return [col for col in data.columns if re.match(r\"(\"+oldCol+\"_)(\\d)\", col)]\n",
    "\n",
    "def dedummify(data, oldCol):\n",
    "    return data[getColsMatching(data,oldCol)].idxmax(axis=1).apply(lambda x : x.split(\"_\")[-1])\n",
    "\n",
    "def Fit(cluster, train, test, model_simple_step2):\n",
    "    \n",
    "    #Get only the data for the required cluster in train and test set\n",
    "    train_region_label= train.loc[getFilterRegion(cluster,train)]\n",
    "    test_region_label= test.loc[getFilterRegion(cluster,test)]\n",
    "    \n",
    "    #removing the region columns in order to force the tree alghoritm to do not split for regions\n",
    "    cols_not_for_step1 = ['Region_PopulationK','Region_AreaKM2','Region_GDP',*cluster]\n",
    "    train_region_columns = train_region_label[cols_not_for_step1]\n",
    "    train_region_label=train_region_label.drop(columns=cols_not_for_step1)\n",
    "\n",
    "    test_region_columns = test_region_label[cols_not_for_step1]\n",
    "    test_region_label=test_region_label.drop(columns=cols_not_for_step1)\n",
    "    \n",
    "    train_x_region_label_step1 = train_region_label.drop(columns=['NumberOfSales','NumberOfCustomers'])\n",
    "    test_x_region_label_step1 = test_region_label.drop(columns=['NumberOfSales','NumberOfCustomers'])\n",
    "    train_y_region_label_step1 = pd.DataFrame(data = train_region_label['NumberOfCustomers'])\n",
    "    test_y_region_label_step1 = pd.DataFrame(data = test_region_label['NumberOfCustomers']) \n",
    "    \n",
    "    #First model predicting NumberOfCustomers\n",
    "    model_simple = RandomForestRegressor()\n",
    "    model_simple = model_simple.fit(train_x_region_label_step1, train_y_region_label_step1)\n",
    "    yp = model_simple.predict(test_x_region_label_step1) #yp=predicted customers del test\n",
    "    \n",
    "    test_region_label[cols_not_for_step1] = test_region_columns[cols_not_for_step1]\n",
    "    train_region_label[cols_not_for_step1] = train_region_columns[cols_not_for_step1]\n",
    "    \n",
    "    # Prepare data for step2\n",
    "    train_x_step2 = train.drop(columns=['NumberOfSales'])\n",
    "    train_y_step2 = pd.DataFrame(data = train['NumberOfSales'])\n",
    "    test_x_region_label_step2 = test_region_label.drop(columns=['NumberOfSales','NumberOfCustomers'])\n",
    "    test_x_region_label_step2['NumberOfCustomers']= yp\n",
    "    test_y_region_label_step2 = pd.DataFrame(data = test_region_label['NumberOfSales']) \n",
    "    \n",
    "    cols_for_step2 = [*getColsMatching(train,\"Month\"), *getColsMatching(train,\"Region\"),'NumberOfCustomers','Region_AreaKM2','HasPromotions','IsHoliday','Region_GDP', \"StoreID\"]\n",
    "    train_x_step2 = train_x_step2[cols_for_step2]\n",
    "    test_x_region_label_step2 = test_x_region_label_step2[cols_for_step2]\n",
    "    \n",
    "    #STEP2\n",
    "    if model_simple_step2 == None:\n",
    "        model_simple_step2 = RandomForestRegressor()\n",
    "        model_simple_step2 = model_simple_step2.fit(train_x_step2, train_y_step2)\n",
    "    yp2 = model_simple_step2.predict(test_x_region_label_step2) #yp2 = le sales predette alla fine\n",
    "    \n",
    "    # Restore Month columns after being dummified to ease the error estimation\n",
    "    test_x_region_label_step2[\"Month\"]=dedummify(test_x_region_label_step2, r\"Month\")\n",
    "    # Prepare needed data\n",
    "    check = pd.DataFrame(test_y_region_label_step2)\n",
    "    check[\"NumberOfPredictedSales\"] = yp2\n",
    "    check[\"StoreID\"] = test_x_region_label_step2[[\"StoreID\"]]\n",
    "    check['Month'] = test_x_region_label_step2['Month']\n",
    "    check[cluster] = test_region_columns[cluster]\n",
    "    \n",
    "    errs = []\n",
    "    for region_label in cluster:\n",
    "        considered_region = (test_x_region_label_step2[region_label]==1).tolist()\n",
    "        check2 = check[considered_region]\n",
    "        check2[region_label]=1\n",
    "        errs.append(RegionError(region_label, check2))\n",
    "        print(\"Region \" + (region_label.split(\"_\")[-1]) + \" : \", errs[-1])\n",
    "    return model_simple_step2, errs\n",
    "\n",
    "def stepwise_selection(X, y, \n",
    "                       initial_list=[], \n",
    "                       threshold_in=0.01, \n",
    "                       threshold_out = 0.05, \n",
    "                       verbose=True):\n",
    "    \"\"\" Perform a forward-backward feature selection \n",
    "    based on p-value from statsmodels.api.OLS\n",
    "    Arguments:\n",
    "        X - pandas.DataFrame with candidate features\n",
    "        y - list-like wstepwise_selectionith the target\n",
    "        initial_list - list of features to start with (column names of X)\n",
    "        threshold_in - include a feature if its p-value < threshold_in\n",
    "        threshold_out - exclude a feature if its p-value > threshold_out\n",
    "        verbose - whether to print the sequence of inclusions and exclusions\n",
    "    Returns: list of selected features \n",
    "    Always set threshold_in < threshold_out to avoid infinite looping.\n",
    "    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    while True:\n",
    "        changed=False\n",
    "        # forward step\n",
    "        excluded = list(set(X.columns)-set(included))\n",
    "        new_pval = pd.Series(index=excluded)\n",
    "        for new_column in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(pd.DataFrame( X.astype(float)[included+[new_column]]))).fit()\n",
    "            new_pval[new_column] = model.pvalues[new_column]\n",
    "        best_pval = new_pval.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pval.argmin()\n",
    "            included.append(best_feature)\n",
    "            changed=True\n",
    "            if verbose:\n",
    "                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n",
    "\n",
    "        # backward step\n",
    "        model = sm.OLS(y, sm.add_constant(pd.DataFrame( X.astype(float)[included]))).fit()\n",
    "        # use all coefs except intercept\n",
    "        pvalues = model.pvalues.iloc[1:]\n",
    "        worst_pval = pvalues.max() # null if pvalues is empty\n",
    "        if worst_pval > threshold_out:\n",
    "            changed=True\n",
    "            worst_feature = pvalues.argmax()\n",
    "            included.remove(worst_feature)\n",
    "            if verbose:\n",
    "                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n",
    "        if not changed:\n",
    "            break\n",
    "    return included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "sales = sales_string_date=pd.read_csv('train.csv')\n",
    "\n",
    "# Removing tuples where stores are closed\n",
    "sales = sales[sales['IsOpen'] == 1]\n",
    "\n",
    "# Converting to category columns that are labels\n",
    "for el in [\"StoreID\", \"Region\"]:\n",
    "    sales[el] = sales[el].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nulls = sales.isnull().sum()\n",
    "sorted([(x,y) for (x,y) in zip(nulls.index, nulls) if y>0], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that Max_Gust_SpeedKm_h has 409947 missing values. We decided not to impute it.\n",
    "\n",
    "Let's start with imputation of \"Events\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "null_Events = sales['Events'].isnull()\n",
    "event_missing = sales[null_Events]\n",
    "event_missing.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['Events'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By this, we discovered that when Event is null the weather is good, from the fact that Precipitationmm mean is almost 0.\n",
    "Furthermore, all the labels of Events are related to bad weather, that means that when no precipitation occurs the label is null.\n",
    "We will impute Event by replacing the missing values with \"Not Specified\" (later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we impute \"CloudCover\", making a distinction when it misses along with Events and when it misses on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_Events = sales['Events'].isnull()\n",
    "null_CloudCover = sales['CloudCover'].isnull()\n",
    "cloudcover_missing = sales[(null_Events)]\n",
    "null_Events = sales['Events'].isnull()\n",
    "event_missing = sales[null_Events]\n",
    "event_cc_missing = sales[null_CloudCover & null_Events]\n",
    "\n",
    "cloudcover_missing.shape, event_missing.shape, event_cc_missing.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 28k tuples where both \"Events\" and \"CloudCover\" are missing, that means that the weather should be good.\n",
    "for the remaining (41k-28k) (i.e. where \"Events\" is not null!) tuples we impute the CloudCoverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_notmissing_cc_missing = sales[null_CloudCover & ~null_Events]\n",
    "#computing mean where CloudCover is not null\n",
    "mean_CC = sales[\"CloudCover\"].mean()\n",
    "event_notmissing_cc_missing[\"CloudCover\"] = event_notmissing_cc_missing[\"CloudCover\"].fillna(mean_CC)\n",
    "sales = pd.concat([sales[~null_CloudCover | null_Events], event_notmissing_cc_missing])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we impute the remaining rows with CloudCover missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_Events = sales['Events'].isnull()\n",
    "CloudyButNotEvent = sales[null_Events]\n",
    "#there are some tuples with no Events but with the attribute CloudCover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_cloudCover = sales[sales['CloudCover'].isnull()]\n",
    "mean_cloudCover = sales[\"CloudCover\"].mean()\n",
    "null_cloudCover[\"CloudCover\"] = null_cloudCover[\"CloudCover\"].fillna(mean_cloudCover)\n",
    "\n",
    "sales = pd.concat([sales[~sales['CloudCover'].isnull()], null_cloudCover])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we impute min,max,mean_VisibilityKm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if are all the same rows where visibility data are missing => yes, they are\n",
    "sales[[\"Max_VisibilityKm\", \"Mean_VisibilityKm\", \"Min_VisibilitykM\"]].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_visibility = sales[sales['Max_VisibilityKm'].isnull()]\n",
    "mean_vis_max = sales[\"Max_VisibilityKm\"].mean()\n",
    "mean_vis_mean = sales[\"Mean_VisibilityKm\"].mean()\n",
    "mean_vis_min = sales[\"Min_VisibilitykM\"].mean()\n",
    "\n",
    "null_visibility[\"Max_VisibilityKm\"] = null_visibility[\"Max_VisibilityKm\"].fillna(mean_vis_max)\n",
    "null_visibility[\"Mean_VisibilityKm\"] = null_visibility[\"Mean_VisibilityKm\"].fillna(mean_vis_mean)\n",
    "null_visibility[\"Min_VisibilitykM\"] = null_visibility[\"Min_VisibilitykM\"].fillna(mean_vis_min)\n",
    "\n",
    "sales = pd.concat([sales[~sales['Max_VisibilityKm'].isnull()], null_visibility])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we impute \"Events\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales=sales.replace(np.nan,'NotSpecified', regex=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sales.quantile(.99).sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "q(\"NearestCompetitor\", .95, lambda x, y: x > y)\n",
    "q(\"Precipitationmm\", .95, lambda x, y: x > y)\n",
    "q(\"Max_Wind_SpeedKm_h\", .95, lambda x,y: x > y)\n",
    "q(\"Max_Wind_SpeedKm_h\", .03, lambda x,y: x < y)\n",
    "q(\"Max_TemperatureC\", .95, lambda x,y: x > y)\n",
    "q(\"Max_TemperatureC\", .03, lambda x,y: x < y)\n",
    "q(\"Min_TemperatureC\", .95, lambda x,y: x > y)\n",
    "q(\"Min_TemperatureC\", .03, lambda x,y: x < y)\n",
    "q(\"Mean_Dew_PointC\", .95, lambda x,y: x > y)\n",
    "q(\"Mean_Dew_PointC\", .05, lambda x,y: x < y)\n",
    "q(\"Mean_Dew_PointC\", .95, lambda x,y: x > y)\n",
    "q(\"Mean_Dew_PointC\", .05, lambda x,y: x < y)\n",
    "q(\"Mean_Humidity\", .95, lambda x,y: x > y)\n",
    "q(\"Mean_Humidity\", .03, lambda x,y: x < y)\n",
    "q(\"Min_VisibilitykM\", .95, lambda x,y: x > y)\n",
    "q(\"Min_Humidity\", .03, lambda x,y: x < y)\n",
    "q(\"Min_Humidity\", .95, lambda x,y: x > y)\n",
    "q(\"Mean_Wind_SpeedKm_h\", .95, lambda x,y: x > y)\n",
    "q(\"Mean_TemperatureC\", .03, lambda x,y: x < y)\n",
    "q(\"Mean_TemperatureC\", .95, lambda x,y: x > y)\n",
    "q(\"Mean_VisibilityKm\", .05, lambda x,y: x < y)\n",
    "q(\"Mean_VisibilityKm\", .95, lambda x,y: x > y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization of Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the numerical features\n",
    "numeric_feats = sales.dtypes[sales.dtypes != \"object\"].index\n",
    "# compute the skewness but only for non missing variables (we already imputed them but just in case ...)\n",
    "skewed_feats = sales[numeric_feats].apply(lambda x: skew(x.dropna()))\n",
    "\n",
    "skewness = pd.DataFrame({\"Variable\":skewed_feats.index, \"Skewness\":skewed_feats.data})\n",
    "# select the variables with a skewness above a certain threshold\n",
    "skewness = skewness.sort_values('Skewness', ascending=[0])\n",
    "f, ax = plt.subplots(figsize=(8,6))\n",
    "plt.xticks(rotation='90')\n",
    "sns.barplot(x=skewness['Variable'], y=skewness['Skewness'])\n",
    "plt.ylim(0,25)\n",
    "plt.xlabel('Numerical Variables', fontsize=15)\n",
    "plt.ylabel('Skewness', fontsize=15)\n",
    "plt.title('', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_feats = skewed_feats[skewed_feats > 0.75]\n",
    "skewed_feats = skewed_feats.drop(['IsHoliday','NumberOfCustomers', \"NumberOfSales\"])\n",
    "sales[skewed_feats.index] = np.log1p(sales[skewed_feats.index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation Analysis and Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By analyzing the correlation, we see that the following variables do not add any additional information. \n",
    "Max_Dew_PointC, Min_Dew_PointC, Max_Sea_Level_PressurehPa, Mean_Sea_Level_PressurehPa, Max_Gust_SpeedKm_h\n",
    "Finally, we drop 'NumberOfCustomers' because is not present in the submission dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales=sales.drop(columns=['Max_Dew_PointC','Min_Dew_PointC','Max_Sea_Level_PressurehPa','Mean_Sea_Level_PressurehPa','Max_Gust_SpeedKm_h'])\n",
    "sales.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop of the row where IsOpen==0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales=sales[sales['IsOpen']==1]\n",
    "sales=sales.drop(columns=['IsOpen'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Date to weekday label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[\"Date\"] = sales[\"Date\"].apply(toDate)\n",
    "sales[\"Day_Of_Week\"] = sales[\"Date\"].astype(\"datetime64\").dt.weekday_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Date to month label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales[\"Month\"] = sales[\"Date\"].astype(\"datetime64\").dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding info about avgsales per month ecc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgSalesForStoreIDForMonth = sales \n",
    "avgSalesForStoreIDForMonth = avgSalesForStoreIDForMonth.groupby(['StoreID','Month'], as_index=False)['NumberOfSales'].mean() \n",
    "avgSalesForStoreIDForMonth = avgSalesForStoreIDForMonth.rename(index=str, columns={\"NumberOfSales\": \"AvgSalesForMonth\"})\n",
    "\n",
    "varSalesForStoreIDForMonth = sales \n",
    "varSalesForStoreIDForMonth = varSalesForStoreIDForMonth.groupby(['StoreID','Month'], as_index=False)['NumberOfSales'].var() \n",
    "varSalesForStoreIDForMonth = varSalesForStoreIDForMonth.rename(index=str, columns={\"NumberOfSales\": \"VarSalesForMonth\"})\n",
    "\n",
    "avgCustomersForStoreIDForMonth = sales \n",
    "avgCustomersForStoreIDForMonth = avgCustomersForStoreIDForMonth.groupby(['StoreID','Month'], as_index=False)['NumberOfCustomers'].mean() \n",
    "avgCustomersForStoreIDForMonth = avgCustomersForStoreIDForMonth.rename(index=str, columns={\"NumberOfCustomers\": \"AvgCustomersForMonth\"})\n",
    "\n",
    "varCustomersorStoreIDForMonth = sales \n",
    "varCustomersorStoreIDForMonth = varCustomersorStoreIDForMonth.groupby(['StoreID','Month'], as_index=False)['NumberOfCustomers'].var() \n",
    "varCustomersorStoreIDForMonth = varCustomersorStoreIDForMonth.rename(index=str, columns={\"NumberOfCustomers\": \"VarCustomersForMonth\"})\n",
    "\n",
    "sales = sales.merge(avgSalesForStoreIDForMonth, left_on=['StoreID','Month'], right_on = ['StoreID','Month']) \n",
    "sales = sales.merge(varSalesForStoreIDForMonth, left_on=['StoreID','Month'], right_on = ['StoreID','Month'])\n",
    "sales = sales.merge(avgCustomersForStoreIDForMonth, left_on=['StoreID','Month'], right_on = ['StoreID','Month']) \n",
    "sales = sales.merge(varCustomersorStoreIDForMonth, left_on=['StoreID','Month'], right_on = ['StoreID','Month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummify variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = pd.get_dummies(sales, columns=['StoreType','Events','AssortmentType', \"Region\", \"Day_Of_Week\",\"Month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Definition\n",
    "Separating the last 2 months, and use those as a test set and comparing the total of the predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_train = date(2018, 1, 1)\n",
    "train = sales[sales[\"Date\"] - start_train < timedelta(0)]\n",
    "test = sales[sales[\"Date\"] - start_train > timedelta(days=1)]\n",
    "train = train.drop(columns=[\"Date\"])\n",
    "test = test.drop(columns=[\"Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clusterization of regions which are similar or with too few samples to be fitted on their own.\n",
    "Used t-sne to visualize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Region_labels = [['Region_0', 'Region_1', 'Region_4','Region_5','Region_7', 'Region_8', 'Region_10' ], ['Region_2'], ['Region_3'], ['Region_6', 'Region_9']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "errors = []\n",
    "for cluster in Region_labels:\n",
    "    print(\"Cluster : \" + str(cluster))\n",
    "    model, errs = Fit(cluster,train,test, model)\n",
    "    errors+=errs\n",
    "from functools import reduce\n",
    "print(\"Mean Error : \",reduce(lambda x, y: x+y, errors, 0)/len(errors))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "['Region_0', 'Region_1', 'Region_5', 'Region_8', 'Region_10']\n",
    "0.050509596125632474\n",
    "0.047408014394271156\n",
    "0.060103972487105875\n",
    "0.05459679528646284\n",
    "0.03717668271838287\n",
    "['Region_2', 'Region_7']\n",
    "0.06517601843888417\n",
    "0.06410857511405414\n",
    "['Region_3']\n",
    "0.03545359756539263\n",
    "['Region_4', 'Region_6', 'Region_9']\n",
    "0.03287895170508598\n",
    "0.04151854681340043\n",
    "0.05472072215303363"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
